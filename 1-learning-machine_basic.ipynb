{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程--将数据处理为数值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征名称: ['city=上海' 'city=北京' 'city=广州' 'temperature']\n",
      "\n",
      "转换后的特征矩阵:\n",
      "[[ 0.  1.  0. 20.]\n",
      " [ 1.  0.  0. 25.]\n",
      " [ 0.  0.  1. 30.]]\n",
      "\n",
      "转换回字典列表:\n",
      "[{'city=北京': np.float64(1.0), 'temperature': np.float64(20.0)}, {'city=上海': np.float64(1.0), 'temperature': np.float64(25.0)}, {'city=广州': np.float64(1.0), 'temperature': np.float64(30.0)}]\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 创建示例数据\n",
    "data = [\n",
    "    {'city': '北京', 'temperature': 20},\n",
    "    {'city': '上海', 'temperature': 25},\n",
    "    {'city': '广州', 'temperature': 30}\n",
    "]\n",
    "\n",
    "# 创建DictVectorizer对象\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# 将字典列表转换为特征矩阵\n",
    "X3 = dv.fit_transform(data)\n",
    "\n",
    "# 打印特征名称\n",
    "print(\"特征名称:\", dv.get_feature_names_out())\n",
    "\n",
    "# 打印转换后的特征矩阵\n",
    "print(\"\\n转换后的特征矩阵:\")\n",
    "print(X3.toarray())\n",
    "\n",
    "# 将特征矩阵转换回字典列表\n",
    "print(\"\\n转换回字典列表:\")\n",
    "print(dv.inverse_transform(X3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表: ['amazing' 'is' 'learn' 'learning' 'love' 'machine' 'to' 'want']\n",
      "\n",
      "词频矩阵:\n",
      "[[0 0 0 1 1 1 0 0]\n",
      " [1 1 0 1 0 1 0 0]\n",
      " [0 0 1 1 0 1 1 1]]\n",
      "\n",
      "每个文档的词频统计:\n",
      "\n",
      "文档 1: I love machine learning\n",
      "[[0 0 0 1 1 1 0 0]]\n",
      "\n",
      "文档 2: Machine learning is amazing\n",
      "[[1 1 0 1 0 1 0 0]]\n",
      "\n",
      "文档 3: I want to learn machine learning\n",
      "[[0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 创建示例文本数据\n",
    "text_data = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I want to learn machine learning\"\n",
    "]\n",
    "\n",
    "# 创建CountVectorizer对象\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# 将文本转换为词频矩阵\n",
    "X2 = cv.fit_transform(text_data)\n",
    "\n",
    "# 打印特征名称（词汇表）\n",
    "print(\"词汇表:\", cv.get_feature_names_out())\n",
    "\n",
    "# 打印词频矩阵\n",
    "print(\"\\n词频矩阵:\")\n",
    "print(X2.toarray())\n",
    "\n",
    "# 打印每个文档的词频统计\n",
    "print(\"\\n每个文档的词频统计:\")\n",
    "for i, doc in enumerate(text_data):\n",
    "    print(f\"\\n文档 {i+1}: {doc}\")\n",
    "    print(cv.transform([doc]).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/1f/d7sgbp1155l8kkgswvhg8frw0000gn/T/jieba.cache\n",
      "Loading model cost 0.246 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 喜欢 机器 学习', '机器 学习 很 有趣', '我 想 学习 机器 学习']\n",
      "词汇表: ['喜欢' '学习' '有趣' '机器']\n",
      "\n",
      "词频矩阵:\n",
      "[[1 1 0 1]\n",
      " [0 1 1 1]\n",
      " [0 2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# jieba分词\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 创建示例中文文本数据\n",
    "chinese_text = [\n",
    "    \"我喜欢机器学习\",\n",
    "    \"机器学习很有趣\",\n",
    "    \"我想学习机器学习\"\n",
    "]\n",
    "\n",
    "# 使用jieba进行分词\n",
    "segmented_text = []\n",
    "for text in chinese_text:\n",
    "    # 使用jieba.cut进行分词\n",
    "    words = jieba.cut(text)\n",
    "    # 将分词结果用空格连接成字符串\n",
    "    segmented_text.append(\" \".join(words))\n",
    "print(segmented_text)\n",
    "\n",
    "# 创建CountVectorizer对象\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# 将分词后的文本转换为词频矩阵\n",
    "X3 = cv.fit_transform(segmented_text)\n",
    "\n",
    "# 打印特征名称（词汇表）\n",
    "print(\"词汇表:\", cv.get_feature_names_out())\n",
    "\n",
    "# 打印词频矩阵\n",
    "print(\"\\n词频矩阵:\")\n",
    "print(X3.toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表: ['喜欢' '学习' '有趣' '机器']\n",
      "\n",
      "TF-IDF矩阵:\n",
      "[[0.76749457 0.45329466 0.         0.45329466]\n",
      " [0.         0.45329466 0.76749457 0.45329466]\n",
      " [0.         0.89442719 0.         0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "# tf-idf(逆文档频率)\n",
    "# 公式：tf-idf(i,j)=tf*idf，tf是词频，idf = log(总文本数/包含词i的文本数)\n",
    "# 通常分母会加1，防止分母为0\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 创建示例中文文本数据\n",
    "chinese_text = [\n",
    "    \"我喜欢机器学习\",\n",
    "    \"机器学习很有趣\",\n",
    "    \"我想学习机器学习\"\n",
    "]\n",
    "\n",
    "# 使用jieba进行分词\n",
    "segmented_text = []\n",
    "for text in chinese_text:\n",
    "    words = jieba.cut(text)\n",
    "    segmented_text.append(\" \".join(words))\n",
    "\n",
    "# 创建TfidfVectorizer对象\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# 将分词后的文本转换为TF-IDF矩阵\n",
    "X_tfidf = tfidf.fit_transform(segmented_text)\n",
    "\n",
    "# 打印特征名称（词汇表）\n",
    "print(\"词汇表:\", tfidf.get_feature_names_out())\n",
    "\n",
    "# 打印TF-IDF矩阵\n",
    "print(\"\\nTF-IDF矩阵:\")\n",
    "print(X_tfidf.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化、标准化、缺失值插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:48:23.404407Z",
     "start_time": "2025-06-17T12:48:22.981362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "归一化后的数据:\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# 使用MinMaxScaler进行归一化\n",
    "# 公式：(x-min)/(max-min)\n",
    "# 对不同样本的同一特征进行归一化，min和max是该特征的最小值和最大值\n",
    "# 对异常值敏感，对异常值进行归一化时，会将其归一化到0或1，导致数据失真\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 创建示例数据\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 对数据进行归一化\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# 打印原始数据和归一化后的数据\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "print(\"\\n归一化后的数据:\")\n",
    "print(X_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "标准化后的数据:\n",
      "[[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# 使用StandardScaler进行标准化\n",
    "# 公式：(x-均值)/标准差\n",
    "# 适合现代嘈杂，高维数据，对异常值不敏感\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 创建示例数据\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# 创建StandardScaler对象\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 对数据进行标准化\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# 打印原始数据和标准化后的数据\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "print(\"\\n标准化后的数据:\")\n",
    "print(X_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "[[ 1. nan  3.]\n",
      " [ 4.  5. nan]\n",
      " [ 7.  8.  9.]]\n",
      "\n",
      "使用均值策略插补后的数据:\n",
      "[[1.  6.5 3. ]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  9. ]]\n",
      "\n",
      "使用中位数策略插补后的数据:\n",
      "[[1.  6.5 3. ]\n",
      " [4.  5.  6. ]\n",
      " [7.  8.  9. ]]\n"
     ]
    }
   ],
   "source": [
    "# 使用SimpleImputer进行缺失值插补\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# 创建包含缺失值的示例数据\n",
    "X = np.array([[1, np.nan, 3],\n",
    "              [4, 5, np.nan],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# 创建SimpleImputer对象，使用均值策略\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# 对数据进行缺失值插补\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# 打印原始数据和插补后的数据\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "print(\"\\n使用均值策略插补后的数据:\")\n",
    "print(X_imputed)\n",
    "\n",
    "# 使用中位数策略的示例\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "X_imputed_median = imputer_median.fit_transform(X)\n",
    "\n",
    "print(\"\\n使用中位数策略插补后的数据:\")\n",
    "print(X_imputed_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征预处理中的VarianceThreshold和PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "[[0 2 0 3]\n",
      " [0 1 4 3]\n",
      " [0 1 1 3]]\n",
      "\n",
      "特征选择后的数据:\n",
      "[[0]\n",
      " [4]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 使用VarianceThreshold进行特征选择\n",
    "# 删除所有低方差特征，即删除方差小于阈值的特征。\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "# 创建示例数据\n",
    "X = np.array([[0, 2, 0, 3],\n",
    "              [0, 1, 4, 3],\n",
    "              [0, 1, 1, 3]])\n",
    "\n",
    "# 创建VarianceThreshold对象，设置方差阈值为0.8\n",
    "selector = VarianceThreshold(threshold=0.8)\n",
    "\n",
    "# 对数据进行特征选择\n",
    "X_selected = selector.fit_transform(X)\n",
    "\n",
    "# 打印原始数据和选择后的数据\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "print(\"\\n特征选择后的数据:\")\n",
    "print(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]]\n",
      "\n",
      "降维后的数据:\n",
      "[[-1.20000000e+01 -4.32120803e-16]\n",
      " [-4.00000000e+00  1.17851128e-16]\n",
      " [ 4.00000000e+00 -1.17851128e-16]\n",
      " [ 1.20000000e+01 -3.53553384e-16]]\n",
      "\n",
      "每个主成分解释的方差比例:\n",
      "[1.00000000e+00 1.06095675e-33]\n",
      "\n",
      "累计方差比例:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 使用PCA进行特征降维\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# 创建示例数据\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12],\n",
    "              [13, 14, 15, 16]])\n",
    "\n",
    "# 创建PCA对象，设置降维后的维度为2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# 对数据进行降维\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# 打印原始数据和降维后的数据\n",
    "print(\"原始数据:\")\n",
    "print(X)\n",
    "print(\"\\n降维后的数据:\")\n",
    "print(X_reduced)\n",
    "\n",
    "# 打印每个主成分解释的方差比例\n",
    "print(\"\\n每个主成分解释的方差比例:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# 打印累计方差比例\n",
    "print(\"\\n累计方差比例:\")\n",
    "print(np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 鸢尾花分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集形状: (150, 4)\n",
      "特征名称: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "目标类别: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "训练集大小: (105, 4)\n",
      "测试集大小: (45, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 导入必要的库\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "\n",
    "# 获取特征和目标变量\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 打印数据集的基本信息\n",
    "print(\"数据集形状:\", X.shape)\n",
    "print(\"特征名称:\", iris.feature_names)\n",
    "print(\"目标类别:\", iris.target_names)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 打印训练集和测试集的大小\n",
    "print(\"\\n训练集大小:\", X_train.shape)\n",
    "print(\"测试集大小:\", X_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
